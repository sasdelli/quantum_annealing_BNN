{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from libsvm.svmutil import *\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import struct\n",
    "import networkx as nx\n",
    "import dwave_networkx as dnx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dimod\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite, FixedEmbeddingComposite\n",
    "from dwave.cloud import Client\n",
    "from dwave.system import DWaveSampler\n",
    "from dwave.embedding import EmbeddedStructure\n",
    "import dwave.inspector\n",
    "import dwave.embedding\n",
    "from minorminer import find_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\n",
    "#! wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Leap_API_token = '' ### from the leap account dashboard at https://cloud.dwavesys.com/\n",
    "DATA = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client.from_config(token=Leap_API_token)\n",
    "\n",
    "solvers = client.get_solvers(num_qubits__gt=3000)\n",
    "solvers\n",
    "\n",
    "solver = solvers[0]\n",
    "solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from( solver.edges )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://pypi.org/project/libsvm/\n",
    "##https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_matrix(x):\n",
    "    x_data = np.zeros((len(x),123))\n",
    "    exists = np.zeros(123)\n",
    "    for i,x_i in enumerate( x ):\n",
    "        for j in range(123):\n",
    "            if j in x_i.keys():\n",
    "                assert x_i[j] == 1\n",
    "                x_data[i,j] = 1.\n",
    "                exists[j] = 1.\n",
    "    return x_data, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_data(sel=3):\n",
    "    \n",
    "    ## selects a subset of features from the a1a dataset\n",
    "    \n",
    "    #global x\n",
    "    global y\n",
    "    #global x_test\n",
    "    global y_test\n",
    "    global select\n",
    "    global x_data_reduced\n",
    "    global x_test_data_reduced\n",
    "    y, x = svm_read_problem(DATA +'/a1a')\n",
    "    y_test, x_test = svm_read_problem(DATA +'/a1a.t')\n",
    "    np.random.seed(2)\n",
    "    train_indexes = list(range(len(y)))\n",
    "    np.random.shuffle(train_indexes)\n",
    "    y = [y[i] for i in train_indexes]\n",
    "    x = [x[i] for i in train_indexes]\n",
    "    x_data, exists = data_to_matrix(x)\n",
    "    x_test_data, exists_test = data_to_matrix(x_test)\n",
    "\n",
    "    if sel== 0.3:\n",
    "        select = np.mean(x_data,0) > .3\n",
    "        np.array(range(123))[select]\n",
    "    elif sel== 7:     ### selects 7 features\n",
    "        select = (((np.mean(x_data,0) > .3) + 0.) + (np.mean(x_data,0) < .65)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "    elif sel== 9:      ### selects 9 features\n",
    "        select = (((np.mean(x_data,0) > .3) + 0.) + (np.mean(x_data,0) < .7)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "        \n",
    "    elif sel== 15:      ### selects 15 features\n",
    "        select = (((np.mean(x_data,0) > .21) + 0.) + (np.mean(x_data,0) < .79)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "        \n",
    "    elif sel == 3:      ### selects 3 features\n",
    "        select = np.array([False for i in range(123)])\n",
    "        select[6] = True\n",
    "        select[22] = True\n",
    "        select[36] = True\n",
    "        select\n",
    "    elif sel== 'all':\n",
    "        select = np.array([True for i in range(123)])\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    x_data_reduced = x_data[:,select]\n",
    "    x_test_data_reduced = x_test_data[:,select]\n",
    "    \n",
    "    \n",
    "    y = 0+(np.array(y) > 0.)\n",
    "    y_test = 0+(np.array(y_test) > 0.)\n",
    "\n",
    "#set_up_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNN code adapted from:\n",
    "# github.com/Akashmathwani/Binarized-Neural-networks-using-pytorch\n",
    "# \n",
    "\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "class BinaryTanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryTanh, self).__init__()\n",
    "        self.hardtanh = nn.Hardtanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.hardtanh(input)\n",
    "        output = binarize(output)\n",
    "        \n",
    "        if num_classes == 2:\n",
    "            output += 1\n",
    "            output /= 2\n",
    "            output = output.squeeze()\n",
    "        return output\n",
    "        \n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "\n",
    "    def forward(self, input):\n",
    "        binary_weight = binarize(self.weight)\n",
    "        if self.bias is None:\n",
    "            return F.linear(input, binary_weight)\n",
    "        else:\n",
    "            return F.linear(input, binary_weight, self.bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Define the NN architecture\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryNet, self).__init__()\n",
    "        self.fc_list = nn.ModuleList()\n",
    "        if num_classes == 2:\n",
    "            output_net_size = 1\n",
    "        else:\n",
    "            output_net_size = num_classes\n",
    "        input_layer_size = input_size\n",
    "        output_layer_size = hidden_n\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            if i == n_layers -1:\n",
    "                output_layer_size = output_net_size\n",
    "            self.fc_list.append( BinaryLinear(input_layer_size, output_layer_size, bias=False) )\n",
    "            input_layer_size = hidden_n\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activation = BinaryTanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, input_size)\n",
    "        # add hidden layer, with relu activation function\n",
    "        for fc_layer in self.fc_list:\n",
    "            x = self.activation(fc_layer(x))\n",
    "        return x\n",
    "\n",
    "# # initialize the NN\n",
    "# model = BinaryNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubovert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_qubo_model(layers_to_optimize=[1,2]):\n",
    "    global qubo_vars\n",
    "    global H\n",
    "    global image_index\n",
    "    global H_layers_to_optim\n",
    "    H_layers_to_optim = layers_to_optimize\n",
    "    qubo_vars = {} ## dict of variables\n",
    "    for j_layer in layers_to_optimize:   ## second and third layer\n",
    "        for i in range( getattr(model, 'fc_list')[j_layer].weight.shape[0] ):\n",
    "            for k in range( getattr(model, 'fc_list')[j_layer].weight.shape[1] ):\n",
    "                qubo_vars[f'weight_{j_layer}_{i}_{k}'] = boolean_var( f'weight_{j_layer}_{i}_{k}' )\n",
    "    H = PCBO()\n",
    "    image_index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constrains_qubo( input_x, gt, Lambda=1):\n",
    "    target_index_i = np.zeros(num_classes)\n",
    "    target_index_i[int(gt)] = 1.\n",
    "    global qubo_vars\n",
    "    global H\n",
    "    global image_index\n",
    "    for k_layer_i in range(len(H_layers_to_optim)):\n",
    "        k_layer = H_layers_to_optim[k_layer_i]\n",
    "        if k_layer_i == 0:\n",
    "            fc_in = input_x\n",
    "            \n",
    "        ### loop over output dimension of layer\n",
    "        for j in range(  getattr(model, 'fc_list')[k_layer].weight.shape[0]  ):\n",
    "            partial = 0.\n",
    "            count = 0.\n",
    "            ### loop over input dimension of layer\n",
    "            for i in range( getattr(model, 'fc_list')[k_layer].weight.shape[1] ):   ## loop over input dimension of layer\n",
    "                #print( k_layer_i, j, i, image_index)\n",
    "                qubo_vars[f'partial_matrix_product_{k_layer}_{i}_{j}_{image_index}'] = boolean_var(f'partial_matrix_product_{k_layer}_{i}_{j}_{image_index}')\n",
    "                if k_layer_i == 0:\n",
    "                    ### the first layer does not require xnor\n",
    "                    if fc_in[i] == 1:\n",
    "                        #print('fc_in 1', k_layer_i, j, i, image_index)\n",
    "                        #print('fc_in', fc_in[i])\n",
    "                        H.add_constraint_eq_BUFFER(\n",
    "                                    qubo_vars[f'partial_matrix_product_{k_layer}_{i}_{j}_{image_index}'],\n",
    "                                    qubo_vars[f'weight_{k_layer}_{j}_{i}'],\n",
    "                                    #lam=Lambda/getattr(model, 'fc_list')[k_layer].weight.shape[0]\n",
    "                                    lam=Lambda\n",
    "                                                  )\n",
    "                    if fc_in[i] == 0:\n",
    "                        #print('fc_in 0', k_layer_i, j, i, image_index)\n",
    "                        #print('fc_in', fc_in[i])\n",
    "                        H.add_constraint_eq_NOT( \n",
    "                                     qubo_vars[f'partial_matrix_product_{k_layer}_{i}_{j}_{image_index}'],\n",
    "                                     qubo_vars[f'weight_{k_layer}_{j}_{i}'],\n",
    "                                     lam=Lambda\n",
    "                                               )\n",
    "                else:\n",
    "                    H.add_constraint_eq_XNOR(\n",
    "                                    qubo_vars[f'partial_matrix_product_{k_layer}_{i}_{j}_{image_index}'],\n",
    "                                    qubo_vars[f'weight_{k_layer}_{j}_{i}'],\n",
    "                                    fc_in[i] ,\n",
    "                                    lam=Lambda\n",
    "                                            )\n",
    "                partial += qubo_vars[f'partial_matrix_product_{k_layer}_{i}_{j}_{image_index}']\n",
    "                count +=1.\n",
    "            qubo_vars[f'matrix_product_{k_layer}_{j}_{image_index}'] = boolean_var(f'matrix_product_{k_layer}_{j}_{image_index}')\n",
    "            print('partial', partial)\n",
    "            ## the activation function is a threshold\n",
    "            add_constraint_greater_zero( count,\n",
    "                                        partial,\n",
    "                                        output_bool=qubo_vars[f'matrix_product_{k_layer}_{j}_{image_index}'],\n",
    "                                        lam=Lambda,\n",
    "                                        k_layer=k_layer,\n",
    "                                        j=j,\n",
    "                                        image_index=image_index,\n",
    "                                       )\n",
    "\n",
    "        fc_in = [ qubo_vars[f'matrix_product_{k_layer}_{j}_{image_index}'] for j in range( getattr(model, 'fc_list')[k_layer].weight.shape[0] ) ]\n",
    "    print(fc_in)\n",
    "    partial = 0.\n",
    "    count = 0.\n",
    "    k_layer = 'loss'\n",
    "    if num_classes == 2:\n",
    "        qubo_vars[f'matrix_product_{k_layer}_{0}_{image_index}'] = boolean_var(f'matrix_product_{k_layer}_{0}_{image_index}')\n",
    "        if gt == 0:\n",
    "            print('gt_0', image_index, fc_in)\n",
    "            H.add_constraint_eq_BUFFER(  qubo_vars[f'matrix_product_{k_layer}_{0}_{image_index}'],  fc_in[0] , lam=Lambda)\n",
    "        if gt == 1:\n",
    "            print('gt_1', image_index, fc_in)\n",
    "            H.add_constraint_eq_NOT(  qubo_vars[f'matrix_product_{k_layer}_{0}_{image_index}'], fc_in[0] , lam=Lambda)\n",
    "        H += qubo_vars[f'matrix_product_{k_layer}_{0}_{image_index}']   \n",
    "        print(f'matrix_product_{k_layer}_{0}_{image_index}')\n",
    "    else:\n",
    "        ## not implemented\n",
    "        assert False\n",
    "    image_index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubovert.sim import anneal_pubo, anneal_qubo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_solution(qubo, x):\n",
    "    result = 0\n",
    "    for (i,j) in qubo:\n",
    "        result += qubo[(i,j)]*x[i]*x[j]\n",
    "    return result\n",
    "#evaluate_solution(target_qubo[0], target_qubo_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def feed_forward_qubo_nn(H_solution, test_data):\n",
    "    \n",
    "    #H_solution needs to be a dictionary like:\n",
    "    # {'partial_matrix_product_0_0_0_0': 1.0,\n",
    "    # 'weight_0_0_0': 0.0,\n",
    "    # 'weight_0_0_1': 1.0,\n",
    "    # 'partial_matrix_product_0_1_0_0': 0.0,\n",
    "    #  ...\n",
    "    # \n",
    "    \n",
    "    plot_images = False\n",
    "    \n",
    "    data = test_data\n",
    "    #target = y_test_data\n",
    "    \n",
    "    test_images = data.shape[0]\n",
    "    if num_classes == 2:\n",
    "        output_net_size = 1\n",
    "    else:\n",
    "        output_net_size = num_classes\n",
    "\n",
    "    input_layer_size = input_size\n",
    "    output_layer_size = hidden_n\n",
    "    w_list = []\n",
    "    for layer in range(n_layers):\n",
    "        if layer == n_layers-1:\n",
    "            output_layer_size = output_net_size\n",
    "        w_list.append(\n",
    "                     np.array([[H_solution[f'weight_{layer}_{j}_{i}'] for i in range(input_layer_size)] for j in range(output_layer_size)])\n",
    "                     )\n",
    "        input_layer_size = hidden_n    \n",
    "    for layer in range(n_layers):\n",
    "        w_list[layer] *=2\n",
    "        w_list[layer] -=1\n",
    "    if plot_images:\n",
    "        for layer in range(n_layers):\n",
    "            plt.figure()\n",
    "            plt.imshow(w_list[layer], cmap='gray')    \n",
    "    out = []\n",
    "    for index_i in range(test_images):\n",
    "        in_x = data[index_i].flatten()\n",
    "        in_x = (in_x > 0.5)*2 -1\n",
    "        for layer in range(n_layers):\n",
    "            in_x = w_list[layer] @ in_x      \n",
    "            in_x = (np.array(in_x) > 0)*2 -1  ## binarize\n",
    "        out.append( in_x )\n",
    "    out = np.array(out)\n",
    "    results = (out > 0.5)*1.\n",
    "    results = results.reshape((test_data.shape[0]) )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as grb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grb_train = []\n",
    "def mycallback_time(model, where):\n",
    "    if where == grb.GRB.Callback.MIP:\n",
    "        _time = model.cbGet(grb.GRB.Callback.RUNTIME)\n",
    "        best = model.cbGet(grb.GRB.Callback.MIP_OBJBST)\n",
    "        cur_bd = model.cbGet(grb.GRB.Callback.MIP_OBJBND)\n",
    "\n",
    "        grb_train.append((_time,best, cur_bd))\n",
    "        if _time > grb_time and best < grb.GRB.INFINITY:\n",
    "            model.terminate()\n",
    "            print(_time, best, cur_bd)\n",
    "\n",
    "def fix_zero(x):\n",
    "    outx=[]\n",
    "    for i in x:\n",
    "        if i == 0:\n",
    "            outx.append(-1)\n",
    "        else:\n",
    "            outx.append(i)\n",
    "    return outx\n",
    "def mycallback_plot(model, where):\n",
    "    if where == grb.GRB.Callback.MIP:\n",
    "        time = model.cbGet(grb.GRB.Callback.RUNTIME)\n",
    "        best = model.cbGet(grb.GRB.Callback.MIP_OBJBST)\n",
    "        cur_bd = model.cbGet(grb.GRB.Callback.MIP_OBJBND)\n",
    "        time_ = int(time)\n",
    "        global print_time_\n",
    "        if print_time_ != time_:\n",
    "            print(time, best, cur_bd)\n",
    "            print_time_ = time_\n",
    "        grb_train.append((time,best, cur_bd))\n",
    "        if time > grb_time:\n",
    "            model.terminate()\n",
    "            \n",
    "    if where == grb.GRB.Callback.MIPSOL:\n",
    "        # MIP solution callback\n",
    "        solution = model.cbGetSolution(model._Model__vars)\n",
    "        plt.figure()\n",
    "        plt.imshow(np.array([fix_zero(solution)]), cmap='gray')\n",
    "print_time_ = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_grb_model_qubo(target_qubo_0, const=0):\n",
    "\n",
    "    global grb_model\n",
    "    global gurobi_vars\n",
    "    global grb_loss\n",
    "    grb_model = grb.Model()\n",
    "    grb_model.setParam('OutputFlag', False)\n",
    "    grb_model.setParam('Threads', 1)\n",
    "\n",
    "    # First add the input variables as Gurobi variables.\n",
    "    gurobi_vars = []\n",
    "    v = grb_model.addVar( vtype=grb.GRB.BINARY, name=f'x')\n",
    "    grb_model.update()\n",
    "    gurobi_vars.append(v)\n",
    "\n",
    "\n",
    "    grb_loss = 0\n",
    "    grb_model.update()\n",
    "\n",
    "    for (i,j) in target_qubo_0:\n",
    "        v = grb_model.getVarByName(f\"{i}\")\n",
    "        if v is None:\n",
    "            v = grb_model.addVar( vtype=grb.GRB.BINARY, name=f'{i}')\n",
    "            gurobi_vars.append(v)\n",
    "        grb_model.update()\n",
    "        w = grb_model.getVarByName(f\"{j}\")\n",
    "        if w is None:\n",
    "            w = grb_model.addVar( vtype=grb.GRB.BINARY, name=f'{j}')\n",
    "            gurobi_vars.append(w)\n",
    "        grb_model.update()\n",
    "        grb_loss += v*w*target_qubo_0[(i,j)]\n",
    "        grb_model.update()\n",
    "    grb_loss += const\n",
    "    grb_model.update()\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_gurobi_model_hard_c(layers_to_optimize=[0,1]):\n",
    "    global grb_model_hard_c\n",
    "    global gurobi_vars_hard_c\n",
    "    global grb_loss_hard_c\n",
    "    global eps\n",
    "    global layers_to_optim\n",
    "    layers_to_optim = layers_to_optimize\n",
    "    grb_model_hard_c = grb.Model()\n",
    "    grb_model_hard_c.setParam('OutputFlag', False)\n",
    "    grb_model_hard_c.setParam('Threads', 1)\n",
    "\n",
    "    grb_loss_hard_c = 0\n",
    "    \n",
    "    # First add the input variables as Gurobi variables.\n",
    "    gurobi_vars_hard_c = []\n",
    "    #binary_vars = []\n",
    "    for j_layer in layers_to_optimize:   ## second and third layer\n",
    "        for i in range( np.prod( getattr(model, 'fc_list')[j_layer].weight.shape ) ):\n",
    "            v = grb_model_hard_c.addVar( vtype=grb.GRB.BINARY,\n",
    "                                          name=f'N_{j_layer}_{i}')\n",
    "            c = grb_model_hard_c.addVar(vtype=grb.GRB.CONTINUOUS, lb=-1, name= f'spin_{j_layer}_{i}')\n",
    "            grb_model_hard_c.addConstr( c == v*2 -1 )\n",
    "            gurobi_vars_hard_c.append(c)\n",
    "\n",
    "            grb_loss_hard_c += c*weights_biases_hard_c[f'spin_{j_layer}_{i}'] ## bias term for online training\n",
    "\n",
    "    grb_model_hard_c.update()\n",
    "    \n",
    "    eps = 1e-5\n",
    "\n",
    "def add_constrains_hard_c( input_x, gt , batch_index, approximate_constr=False):\n",
    "    if approximate_constr != False:\n",
    "        Lambda = approximate_constr\n",
    "        \n",
    "    target_index_i = [(gt>0)*2-1]\n",
    "    num_classes = 1\n",
    "\n",
    "    \n",
    "    global gurobi_vars_hard_c\n",
    "    layer_start = 0\n",
    "    for k_layer_i in range(len(layers_to_optim)):\n",
    "        k_layer = layers_to_optim[k_layer_i]\n",
    "        if k_layer_i == 0:\n",
    "            fc_in = (input_x>0)*2-1\n",
    "        else:\n",
    "            layer_start += np.prod(getattr(model, 'fc_list')[layers_to_optim[k_layer_i-1]].weight.shape)\n",
    "            fc_in =  gurobi_vars_hard_c[-getattr(model, 'fc_list')[k_layer].weight.shape[1] : ]\n",
    "        for i in range(  getattr(model, 'fc_list')[k_layer].weight.shape[0]  ):\n",
    "            v = grb_model_hard_c.addVar( vtype=grb.GRB.BINARY,\n",
    "                                          name=f'N_{batch_index}_{k_layer}_{i}')\n",
    "            c = grb_model_hard_c.addVar(vtype=grb.GRB.CONTINUOUS, lb=-1, name= f'out_{batch_index}_{k_layer}_{i}')\n",
    "            grb_model_hard_c.addConstr( c == v*2 -1 )\n",
    "            gurobi_vars_hard_c.append(c)\n",
    "        grb_model_hard_c.update()\n",
    "        global grb_loss_hard_c\n",
    "        ### loop over output dimension of layer\n",
    "        for j in range(  getattr(model, 'fc_list')[k_layer].weight.shape[0]  ):\n",
    "            partial_matrix_product = []\n",
    "            ### loop over input dimension of layer\n",
    "            for i in range( getattr(model, 'fc_list')[k_layer].weight.shape[1] ):   ## loop over input dimension of layer\n",
    "                b = grb_model_hard_c.addVar(vtype=grb.GRB.CONTINUOUS, lb=-1, name= f'partial_matrix_product_{batch_index}_{k_layer}_{i}_{j}')\n",
    "                grb_model_hard_c.update()\n",
    "                grb_model_hard_c.addConstr( b == gurobi_vars_hard_c[layer_start +i+ getattr(model, 'fc_list')[k_layer].weight.shape[1]*j] * fc_in[i] )\n",
    "                grb_model_hard_c.update()\n",
    "                partial_matrix_product.append(b)\n",
    "                \n",
    "            grb_model_hard_c.addConstr( eps <=  sum(partial_matrix_product) * gurobi_vars_hard_c[-getattr(model, 'fc_list')[k_layer].weight.shape[0] + j] )\n",
    "        grb_model_hard_c.update()\n",
    "\n",
    "    check_vrb_i_abs = grb_model_hard_c.addVar(vtype=grb.GRB.CONTINUOUS, lb=0, name= f'loss_abs_{batch_index}')\n",
    "    check_vrb_i = grb_model_hard_c.addVar(vtype=grb.GRB.CONTINUOUS, lb=-1, name= f'loss_{batch_index}')\n",
    "    grb_model_hard_c.update()\n",
    "    \n",
    "    grb_model_hard_c.addConstr( check_vrb_i ==  (gurobi_vars_hard_c[-1] - target_index_i[0] )/2.)\n",
    "    grb_model_hard_c.update()\n",
    "    grb_model_hard_c.addConstr( check_vrb_i_abs == grb.abs_( check_vrb_i ) )\n",
    "    grb_model_hard_c.update()\n",
    "    grb_loss_hard_c += check_vrb_i_abs\n",
    "    grb_model_hard_c.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_weights_biases(layers_to_optimize=[1,2]):\n",
    "    global weights_biases\n",
    "    \n",
    "    weights_biases = {} ## dict of variables\n",
    "    for j_layer in layers_to_optimize:   ## second and third layer\n",
    "        for i in range( getattr(model, 'fc_list')[j_layer].weight.shape[0] ):\n",
    "            for k in range( getattr(model, 'fc_list')[j_layer].weight.shape[1] ):\n",
    "                weights_biases[f'weight_{j_layer}_{i}_{k}'] = 0.\n",
    "\n",
    "def set_up_weights_biases_hard_c(layers_to_optimize=[1,2]):\n",
    "    global weights_biases_hard_c\n",
    "    \n",
    "    weights_biases_hard_c = {} ## dict of variables\n",
    "    for j_layer in layers_to_optimize:   ## second and third layer\n",
    "        for i in range( getattr(model, 'fc_list')[j_layer].weight.shape[0] ):\n",
    "            for k in range( getattr(model, 'fc_list')[j_layer].weight.shape[1] ):\n",
    "                weights_biases_hard_c[f'spin_{j_layer}_{i*sum(select) + k}'] = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constraint_greater_zero( count, partial_poly, output_bool, lam, k_layer, j, image_index):\n",
    "    global H\n",
    "    ### this should be done in a general way\n",
    "    \n",
    "    if count < 4:\n",
    "        \n",
    "        aux = 1\n",
    "        aux_1 =  boolean_var(f'aux_matrix_product_{k_layer}_{j}_{image_index}_{aux}')\n",
    "        aux_2 = output_bool\n",
    "        H.add_constraint_eq_zero( -aux_1 -2*aux_2 + partial_poly, lam=lam)\n",
    "    elif count < 8:\n",
    "        aux = 1\n",
    "        aux_1 =  boolean_var(f'aux_matrix_product_{k_layer}_{j}_{image_index}_{aux}')\n",
    "        aux = 2\n",
    "        aux_2 =  boolean_var(f'aux_matrix_product_{k_layer}_{j}_{image_index}_{aux}')\n",
    "        aux_4 = output_bool\n",
    "        H.add_constraint_eq_zero( -aux_1 -2*aux_2 -4*aux_4 + partial_poly, lam=lam)\n",
    "    elif count < 16:\n",
    "        aux = 1\n",
    "        aux_1 =  boolean_var(f'aux_matrix_product_{k_layer}_{j}_{image_index}_{aux}')\n",
    "        aux = 2\n",
    "        aux_2 =  boolean_var(f'aux_matrix_product_{k_layer}_{j}_{image_index}_{aux}')\n",
    "        aux = 4\n",
    "        aux_4 =  boolean_var(f'aux_matrix_product_{k_layer}_{j}_{image_index}_{aux}')\n",
    "        aux_8 = output_bool\n",
    "        H.add_constraint_eq_zero( -aux_1 -2*aux_2 -4*aux_4 -8*aux_8 + partial_poly, lam=lam)\n",
    "    elif count < 32:\n",
    "        assert False\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "grb_train = []\n",
    "def train_models():\n",
    "    global list_H_solutions\n",
    "    global H_solution\n",
    "    global weights_biases\n",
    "    global weights_biases_hard_c\n",
    "    global dwave_qubo\n",
    "    global grb_dwave_qubo_solution\n",
    "    global grb_model\n",
    "    global grb_loss\n",
    "    global grb_time\n",
    "    global grb_train\n",
    "    list_H_solutions = []\n",
    "    global list_results\n",
    "    list_results = []\n",
    "\n",
    "    set_up_weights_biases([0,1])\n",
    "    set_up_weights_biases_hard_c([0,1])\n",
    "\n",
    "    for batch_n in range(num_batches_to_run):\n",
    "    #for batch_n in range(int(np.floor(x_data_reduced.shape[0]/selected_to_train))):\n",
    "\n",
    "        print(batch_n)\n",
    "        set_up_qubo_model([0,1])\n",
    "        set_up_gurobi_model_hard_c([0,1]) ### direct gurobi model with hard constraints\n",
    "        for index_i in range(selected_to_train):\n",
    "            in_x = x_data_reduced[batch_n*selected_to_train+index_i]\n",
    "            in_x = 0+(in_x > 0.)\n",
    "            add_constrains_qubo( in_x, y[batch_n*selected_to_train+index_i], Lambda=Lambda_qubo)\n",
    "            add_constrains_hard_c( in_x, y[batch_n*selected_to_train+index_i], batch_index=index_i) ### direct gurobi model with hard constraints\n",
    "        for key in weights_biases.keys():\n",
    "            H[(key,)] += weights_biases[key] ## correct biases with running average of past solutions\n",
    "            ## \n",
    "\n",
    "        print(len(H.variables))\n",
    "        print(len(qubo_vars))                  \n",
    "\n",
    "        # convert to qubo \n",
    "        H_qubo = H.to_qubo()\n",
    "        dwave_qubo = H_qubo.Q   \n",
    "        \n",
    "        if 'embedded_qubo' in optimization_type:\n",
    "            # find dwave embedding of qubo\n",
    "            emb = find_embedding(dwave_qubo, G.edges, random_seed=10) \n",
    "            bqm = dimod.AdjArrayBQM(dwave_qubo, \"BINARY\")\n",
    "            target_bqm = dwave.embedding.embed_bqm(bqm, emb, G.adj) \n",
    "            print(target_bqm.num_variables)\n",
    "            target_qubo = target_bqm.to_qubo()\n",
    "\n",
    "        runtime = time.time()\n",
    "\n",
    "        if optimization_type == 'grb_hard_c':\n",
    "            ### direct gurobi model with hard constraints\n",
    "            grb_model_hard_c.setObjective(grb_loss_hard_c, grb.GRB.MINIMIZE)\n",
    "            grb_train = []\n",
    "            grb_time=np.inf\n",
    "            #grb_model_hard_c.optimize(mycallback_time)\n",
    "            grb_model_hard_c.optimize(mycallback_plot)\n",
    "            plt.figure()\n",
    "            plt.plot([x_[0] for x_ in grb_train][1:],[x_[1] for x_ in grb_train][1:], '-')\n",
    "            plt.plot([x_[0] for x_ in grb_train][10:],[x_[2] for x_ in grb_train][10:], '-')\n",
    "            H_solution={}\n",
    "            for ii in grb_model_hard_c.getVars():\n",
    "                if 'spin' in ii.VarName:\n",
    "                    print(ii.VarName)\n",
    "                    ii.VarName.split('_')\n",
    "                    layer_i = int(ii.VarName.split('_')[1])\n",
    "                    if layer_i == 0:\n",
    "                        _x = int(ii.VarName.split('_')[2]) % sum(select)   ### TO DO: fix this!\n",
    "                        _y = int(ii.VarName.split('_')[2]) // sum(select)\n",
    "                    else:\n",
    "                        _x = int(ii.VarName.split('_')[2]) % hidden_n   ### TO DO: fix this!\n",
    "                        _y = int(ii.VarName.split('_')[2]) // hidden_n\n",
    "                        \n",
    "                    H_solution[f'weight_{layer_i}_{_y}_{_x}'] = (ii.x+1)/2.\n",
    "                    weights_biases_hard_c[ii.VarName] += -alpha* ii.x\n",
    "\n",
    "        elif optimization_type == 'sym_anneal_soft_qubo':\n",
    "            # simulated annealing on pubo:\n",
    "            res = anneal_pubo(H, num_anneals=sym_anneal_num_anneals)\n",
    "            H_solution = res.best.state\n",
    "            print(\"Model value:\", res.best.value)\n",
    "            print(\"Constraints satisfied?\", H.is_solution_valid(H_solution))\n",
    "            \n",
    "        elif optimization_type == 'grb_soft_qubo':\n",
    "            ## gurobi optimization\n",
    "            set_up_grb_model_qubo(dwave_qubo, const=H_qubo[()]) ## set up gurobi model on qubo\n",
    "            \n",
    "            grb_model.setObjective(grb_loss, grb.GRB.MINIMIZE)\n",
    "            grb_train = []\n",
    "            grb_time=np.inf\n",
    "            #grb_model.optimize(mycallback_time)\n",
    "            grb_model.optimize(mycallback_plot)\n",
    "            plt.figure()\n",
    "            plt.plot([x[0] for x in grb_train][:],[x[1] for x in grb_train][:], '-')\n",
    "            plt.plot([x[0] for x in grb_train][:],[x[2] for x in grb_train][:], '-')\n",
    "            #plt.axes().set_yscale('log')\n",
    "            grb_model.getVars()\n",
    "            grb_dwave_qubo_solution={}\n",
    "            for i in grb_model.getVars():\n",
    "                if i.VarName != 'x':\n",
    "                    grb_dwave_qubo_solution[int(i.VarName)] = i.x\n",
    "            H_solution = H.convert_solution(grb_dwave_qubo_solution)\n",
    "            print(\"Constraints satisfied?\", H.is_solution_valid(H_solution))\n",
    "\n",
    "\n",
    "        #else:\n",
    "        #    assert False\n",
    "\n",
    "        if optimization_type == 'grb_soft_embedded_qubo':\n",
    "            ## gurobi optimization\n",
    "            set_up_grb_model_qubo(target_qubo[0]) ## set up gurobi model on qubo\n",
    "            grb_model.setObjective(grb_loss, grb.GRB.MINIMIZE)\n",
    "            grb_train = []\n",
    "            grb_time=np.inf\n",
    "            grb_model.optimize(mycallback_plot)\n",
    "            plt.figure()\n",
    "            plt.plot([x[0] for x in grb_train][:],[x[1] for x in grb_train][:], '-')\n",
    "            plt.plot([x[0] for x in grb_train][:],[x[2] for x in grb_train][:], '-')\n",
    "            #plt.axes().set_yscale('log')\n",
    "            grb_model.getVars()\n",
    "            grb_target_qubo_solution={}\n",
    "            for i in grb_model.getVars():\n",
    "                if i.VarName != 'x':\n",
    "                    grb_target_qubo_solution[int(i.VarName)] = i.x\n",
    "            unembedded_results_from_grb_target_qubo_solution = {}\n",
    "            for i in range(bqm.num_variables):\n",
    "                _results_ = []\n",
    "                for j in emb[i]:\n",
    "                    _results_.append(grb_target_qubo_solution[j])\n",
    "                unembedded_results_from_grb_target_qubo_solution[i] = np.mean(_results_)\n",
    "                assert  np.mean(_results_) == 0 or np.mean(_results_) == 1\n",
    "                #unembedded_results_from_target_qubo_solution[i] = np.random.bit_generator.randbits(1)\n",
    "            #unembedded_results_from_target_qubo_solution\n",
    "            H_solution = H.convert_solution(unembedded_results_from_grb_target_qubo_solution)\n",
    "\n",
    "        if optimization_type == 'sym_anneal_soft_embedded_qubo':\n",
    "            # simulated annealing on embedded qubo\n",
    "            target_qubo_res = anneal_qubo(target_qubo[0], num_anneals=sym_anneal_num_anneals)\n",
    "            target_qubo_solution = target_qubo_res.best.state\n",
    "            print(\"Model value:\", target_qubo_res.best.value)\n",
    "            unembedded_results_from_target_qubo_solution = {}\n",
    "            for i in range(bqm.num_variables):\n",
    "                _results_ = []\n",
    "                for j in emb[i]:\n",
    "                    _results_.append(target_qubo_solution[j])\n",
    "                unembedded_results_from_target_qubo_solution[i] = np.mean(_results_)\n",
    "                #unembedded_results_from_target_qubo_solution[i] = np.random.bit_generator.randbits(1)\n",
    "            H_solution = H.convert_solution(unembedded_results_from_target_qubo_solution)\n",
    "            #unembedded_results_from_target_qubo_solution\n",
    "\n",
    "        qpu_response = 0\n",
    "        if optimization_type == 'quantum_anneal_soft_embedded_qubo':\n",
    "\n",
    "            ## QPU sampler\n",
    "            n_reads = n_reads_qpu\n",
    "            response = solver.sample_bqm(target_bqm, num_reads=n_reads) #, chain_strength=2*L) \n",
    "            response_result = response.result()\n",
    "            unembedded_results = {}\n",
    "            for i in range(bqm.num_variables):\n",
    "                _results_ = []\n",
    "                for j in emb[i]:\n",
    "                    _results_.append(response_result['solutions'][0][j])\n",
    "                unembedded_results[i] = np.mean(_results_)\n",
    "            #unembedded_results\n",
    "            H_solution = H.convert_solution(unembedded_results)\n",
    "            \n",
    "            qpu_response = [ dict(response_result), emb]\n",
    "\n",
    "        runtime = time.time() - runtime\n",
    "\n",
    "        for key in weights_biases.keys():\n",
    "            weights_biases[key] +=  -alpha*(2*H_solution[key]-1)\n",
    "        list_H_solutions.append(H_solution)\n",
    "        \n",
    "        print((feed_forward_qubo_nn( H_solution, x_data_reduced[batch_n*selected_to_train:(batch_n+1)*selected_to_train] ) > 0.5) == y[batch_n*selected_to_train:(batch_n+1)*selected_to_train])\n",
    "        print(feed_forward_qubo_nn( H_solution, x_data_reduced[batch_n*selected_to_train:(batch_n+1)*selected_to_train] ))\n",
    "        print(y[batch_n*selected_to_train:(batch_n+1)*selected_to_train])\n",
    "        wrong_on_train = (sum((feed_forward_qubo_nn( H_solution, x_data_reduced[batch_n*selected_to_train:(batch_n+1)*selected_to_train] ) > 0.5) != y[batch_n*selected_to_train:(batch_n+1)*selected_to_train]))\n",
    "        print(wrong_on_train)\n",
    "        list_results.append( ( batch_n, wrong_on_train, runtime, qpu_response )    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots():\n",
    "    plt.figure()\n",
    "    plt.title('time'+' '+optimization_type)\n",
    "    plt.hist( [ list_results[i][2]-list_results_grb_hard_c[i][2]     for i in range(num_batches_to_run)])\n",
    "    plt.figure()\n",
    "    plt.title('distance from optimum'+' '+optimization_type)\n",
    "    plt.hist( [ list_results[i][1]-list_results_grb_hard_c[i][1]     for i in range(num_batches_to_run)])\n",
    "    plt.figure()\n",
    "    plt.title('distance from optimum vs time'+' '+optimization_type)\n",
    "    plt.plot( [ list_results[i][2]-list_results_grb_hard_c[i][2]     for i in range(num_batches_to_run)],\n",
    "              [ list_results[i][1]-list_results_grb_hard_c[i][1]     for i in range(num_batches_to_run)],\n",
    "              '.'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = 0.\n",
    "Lambda_qubo = 50\n",
    "selected_to_train = 4   ### \"batch size\"\n",
    "\n",
    "#sym_anneal_num_anneals = 4000   ### how many anneals in sym annealing\n",
    "sym_anneal_num_anneals = 1000   ### how many anneals in sym annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up_data(3)             ### keep three data as input\n",
    "input_size = sum(select)\n",
    "hidden_n = 3               ### num hidden layers\n",
    "num_classes = 2            ### binary \n",
    "n_layers = 2               ### num layers\n",
    "num_batches_to_run =  20   ### number of batches to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pytorch NN\n",
    "model = BinaryNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### exp 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'grb_hard_c'\n",
    "train_models()\n",
    "list_results_grb_hard_c = list_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'grb_soft_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'grb_soft_embedded_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'sym_anneal_soft_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'sym_anneal_soft_embedded_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_reads_qpu = 500\n",
    "n_reads_qpu = 5000\n",
    "num_batches_to_run = 20\n",
    "Lambda_qubo = 50\n",
    "optimization_type = 'quantum_anneal_soft_embedded_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_'+'exp_5000_'+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_label = 'exp_1_'\n",
    "alpha = 0.\n",
    "Lambda_qubo = 50\n",
    "selected_to_train = 8   ### \"batch size\"\n",
    "\n",
    "#sym_anneal_num_anneals = 4000   ### how many anneals in sym annealing\n",
    "sym_anneal_num_anneals = 1000   ### how many anneals in sym annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up_data(3)             ### keep three data as input\n",
    "input_size = sum(select)\n",
    "hidden_n = 3               ### num hidden layers\n",
    "num_classes = 2            ### binary \n",
    "n_layers = 2               ### num layers\n",
    "num_batches_to_run =  20   ### number of batches to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pytorch NN\n",
    "model = BinaryNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'grb_hard_c'\n",
    "train_models()\n",
    "list_results_grb_hard_c = list_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+exp_label+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'grb_soft_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+exp_label+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'grb_soft_embedded_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+exp_label+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'sym_anneal_soft_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+exp_label+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_type = 'sym_anneal_soft_embedded_qubo'\n",
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plots()\n",
    "with open('results_'+exp_label+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_reads_qpu = 500\n",
    "num_batches_to_run = 20\n",
    "Lambda_qubo = 50\n",
    "optimization_type = 'quantum_anneal_soft_embedded_qubo'\n",
    "train_models()\n",
    "create_plots()\n",
    "with open('results_'+exp_label+optimization_type+'.pkl', 'wb') as handle:\n",
    "    pickle.dump([list_results,list_results_grb_hard_c], handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
